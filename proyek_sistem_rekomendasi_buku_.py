# -*- coding: utf-8 -*-
"""Proyek Sistem Rekomendasi Buku .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GjLPMqxUEA2pQY2-UbIwe0ghcmfQu5UM

# Proyek Machine Learning  Sistem Rekomendasi- Naomi Sitanggang

## Perkenalan Dataset

Proyek ini menggunakan data yang diambil dari situs Kaggle dengan judul  goodbooks-10k (https://www.kaggle.com/datasets/zygmunt/goodbooks-10k/data). Dataset tersebut memuat sebanyak **lima file** dengan informasi yang dimiliki berbeda di setiap file.

## Import Library
"""

import kagglehub
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""## Memuat Dataset"""

# Download dataset
books_data_path = kagglehub.dataset_download("zygmunt/goodbooks-10k")

# Cek isi direktori
os.listdir(books_data_path)

buku_dengan_tag =  pd.read_csv(os.path.join(books_data_path, 'book_tags.csv'))
informasi_buku = pd.read_csv(os.path.join(books_data_path, 'books.csv'))
rating= pd.read_csv(os.path.join(books_data_path, 'ratings.csv'))
kategori_tag = pd.read_csv(os.path.join(books_data_path, 'tags.csv'))
buku_yang_akan_dibaca = pd.read_csv(os.path.join(books_data_path, 'to_read.csv'))

"""## Exploratory Data Analysis

### Memahami Struktur Data

**book_tags.csv**
"""

print("ğŸ“‘ Struktur Dataset: Buku dengan Tag")
buku_dengan_tag.info()

print("\nğŸ” 5 Data Teratas Buku dengan Tag")
print(buku_dengan_tag.head())

print("\nğŸ§® Jumlah Baris dan Kolom Buku dengan Tag:", buku_dengan_tag.shape)

print("\nâŒ Jumlah Missing Value per Kolom:")
print(buku_dengan_tag.isnull().sum())

print("\nğŸ“„ Jumlah Duplikat Baris:")
print(buku_dengan_tag.duplicated().sum())

print("\nğŸ“Œ Jumlah Duplikat Berdasarkan Tiap Kolom:")
for kolom in buku_dengan_tag.columns:
    jumlah_duplikat_kolom = buku_dengan_tag.duplicated(subset=[kolom]).sum()
    print(f"- {kolom}: {jumlah_duplikat_kolom} duplikat")

"""**Insight:**
   - goodreads_book_id : ID dari goodreads
   - tag_id : ID tag (genre)
   - count : Jumlah goodreads

Tipe data yang dimiliki dari semua variabel merupakan tipe data numerik

**books.csv**
"""

print("ğŸ“‘ Struktur Dataset: Informasi Buku")
informasi_buku.info()

print("\nğŸ” 5 Data Teratas Informasi Buku")
print(informasi_buku.head())

print("\nğŸ§® Jumlah Baris dan Kolom Informasi Buku:", informasi_buku.shape)

print("\nâŒ Jumlah Missing Value per Kolom:")
print(informasi_buku.isnull().sum())

print("\nğŸ“„ Jumlah Duplikat Baris:")
print(informasi_buku.duplicated().sum())

print("\nğŸ“Œ Jumlah Duplikat Berdasarkan Tiap Kolom:")
for kolom in informasi_buku.columns:
    print(f"- {kolom}: {informasi_buku.duplicated(subset=[kolom]).sum()} duplikat")

"""**Insight:**
   - id : ID dari file books
   - book_id : ID buku
   - best_book_id : ID dari buku populer
   - work_id : ID karya
   - books_count : jumlah edisi buku tertentu
   - isbn : nomor isbn
   - authors : nama penulis
   - original_publication_year : tahun terbit buku
   - original_title : judul asli buku
   - title : Judul versi final atau yang digunakan di katalog
   - language_code : Kode bahasa
   - average_rating : Rata-rata rating dari semua pengguna
   - ratings_count :  Total jumlah rating yang diterima buku
   - work_ratings_count : Jumlah rating untuk karya
   - work_text_reviews_count : Jumlah ulasan berbasis teks terhadap karya
   - ratings_1 hingga ratings_5 : Jumlah pengguna yang memberi rating 1 hingga 5 bintang
   - image_url : URL gambar sampul buku
   - small_image_url : URL gambar sampul berukuran kecil


Tipe data yang dimiliki dari semua variabel merupakan tipe data numerik dan kategorik

**ratings.csv**
"""

print("ğŸ“‘ Struktur Dataset: Rating")
rating.info()

print("\nğŸ” 5 Data Teratas Rating")
print(rating.head())

print("\nğŸ§® Jumlah Baris dan Kolom Rating:", rating.shape)

print("\nâŒ Jumlah Missing Value per Kolom:")
print(rating.isnull().sum())

print("\nğŸ“„ Jumlah Duplikat Baris:")
print(rating.duplicated().sum())

print("\nğŸ“Œ Jumlah Duplikat Berdasarkan Tiap Kolom:")
for kolom in rating.columns:
    print(f"- {kolom}: {rating.duplicated(subset=[kolom]).sum()} duplikat")

"""**Insight:**

   - book_id : ID buku
   - user_id : ID Pengguna
   - rating : rating buku

Tipe data yang dimiliki dari semua variabel merupakan tipe data numerik

**tags.csv**
"""

print("ğŸ“‘ Struktur Dataset: Kategori Tag")
kategori_tag.info()

print("\nğŸ” 5 Data Teratas Kategori Tag")
print(kategori_tag.head())

print("\nğŸ§® Jumlah Baris dan Kolom Kategori Tag:", kategori_tag.shape)

print("\nâŒ Jumlah Missing Value per Kolom:")
print(kategori_tag.isnull().sum())

print("\nğŸ“„ Jumlah Duplikat Baris:")
print(kategori_tag.duplicated().sum())

print("\nğŸ“Œ Jumlah Duplikat Berdasarkan Tiap Kolom:")
for kolom in kategori_tag.columns:
    print(f"- {kolom}: {kategori_tag.duplicated(subset=[kolom]).sum()} duplikat")

"""**Insight:**
- tag_id : ID tag (genre)
- tag_name : Nama tag (genre)

Tipe data yang dimiliki dari semua variabel merupakan tipe data numerik dan kategorik

**to_read.csv**
"""

print("ğŸ“‘ Struktur Dataset: Buku yang Akan Dibaca")
buku_yang_akan_dibaca.info()

print("\nğŸ” 5 Data Teratas Buku yang Akan Dibaca")
print(buku_yang_akan_dibaca.head())

print("\nğŸ§® Jumlah Baris dan Kolom Buku yang Akan Dibaca:", buku_yang_akan_dibaca.shape)

print("\nâŒ Jumlah Missing Value per Kolom:")
print(buku_yang_akan_dibaca.isnull().sum())

print("\nğŸ“„ Jumlah Duplikat Baris:")
print(buku_yang_akan_dibaca.duplicated().sum())

print("\nğŸ“Œ Jumlah Duplikat Berdasarkan Tiap Kolom:")
for kolom in buku_yang_akan_dibaca.columns:
    print(f"- {kolom}: {buku_yang_akan_dibaca.duplicated(subset=[kolom]).sum()} duplikat")

"""**Insight:**
- user_id : ID pengguna/pembaca
- book_id : ID buku

Tipe data yang dimiliki dari semua variabel merupakan tipe data numerik
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
sns.histplot(rating['rating'], bins=10, kde=False)
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

""" **Insight:** Berdasarkan grafik distribusi rating buku, terlihat bahwa sebagian besar pengguna memberikan rating yang tinggi, terutama pada skor 4.0, diikuti oleh 5.0 dan 3.0. Sementara itu, rating rendah seperti 1.0 dan 2.0 relatif jarang diberikan. Hal ini mengindikasikan bahwa pengguna cenderung memberikan penilaian positif terhadap buku yang mereka baca, yang bisa disebabkan oleh preferensi terhadap bacaan tertentu atau karena mereka hanya memberi rating pada buku yang benar-benar mereka sukai.


"""

# Hitung jumlah ulasan per buku
ulasan_per_buku = rating.groupby('book_id')['rating'].count().reset_index(name='jumlah_ulasan')
ulasan_terbanyak = ulasan_per_buku.sort_values(by='jumlah_ulasan', ascending=False).head(10)
top_buku = ulasan_terbanyak.merge(informasi_buku, left_on='book_id', right_on='book_id')[['title', 'jumlah_ulasan']]

print("Top 10 Buku dengan Ulasan Terbanyak:")
print(top_buku)

# Gabungkan book_tags dengan tags
tag_populer = buku_dengan_tag.merge(kategori_tag, left_on='tag_id', right_on='tag_id')
tag_count = tag_populer.groupby('tag_name')['count'].sum().reset_index().sort_values(by='count', ascending=False).head(10)

plt.figure(figsize=(10, 6))
sns.barplot(data=tag_count, x='count', y='tag_name')
plt.title('Top 10 Tag yang Paling Sering Digunakan')
plt.xlabel('Jumlah')
plt.ylabel('Tag')
plt.show()

"""**Insight:** Grafik top 10 tag menunjukkan bahwa tag to-read merupakan yang paling sering digunakan oleh pengguna, jauh melampaui tag lainnya seperti currently-reading, favorites, dan fiction. Dominasi tag to-read ini menunjukkan bahwa banyak pengguna menggunakan fitur tagging untuk menandai buku-buku yang ingin mereka baca di masa depan. Hal ini mencerminkan perilaku eksploratif dan minat tinggi pengguna terhadap buku-buku yang belum mereka baca.

# Data Preprocessing

**Menggabungkan tag ID pada Buku**
"""

# Ambil tag_id unik dari kedua sumber
tag_dari_buku = set(buku_dengan_tag['tag_id'].unique())
tag_dari_kategori = set(kategori_tag['tag_id'].unique())

# Gabungkan dua set tag_id
semua_tag = tag_dari_buku.union(tag_dari_kategori)

# Tampilkan hasil
print(f'Jumlah genre unik berdasarkan tag_id: {len(semua_tag)}')

"""**Insight:** Setelah menggabungkan tag ID, kita memiliki 34252 genre yang unik berdasarkan tag.

**Mengetahui Jumlah Rating**
"""

# Menggabungkan data rating dengan informasi buku untuk melihat detail setiap penilaian
rekap_rating_buku = pd.merge(rating, informasi_buku, how='left', on='book_id')
rekap_rating_buku

"""**Memeriksa Missing Value**"""

rekap_rating_buku.isnull().sum()

"""**Insight:** Terdapat banyak missing values pada sebagian besar fitur. Hanya fitur bookID, userID, rating saja yang memiliki 0 missing values, dan variabel yang digunakan nanti hanya book_id, user_id, rating, authors, dan title."""

rekap_rating_buku.groupby('book_id').sum()

"""**Menggabungkan Data dengan Judul, Penulis, dan Tahun Terbit Buku**"""

# Menyimpan data rating awal sebagai referensi ulasan pengguna
data_ulasan = rating.copy()
data_ulasan

# Menyatukan rating dengan informasi spesifik seperti penulis, judul, dan tahun terbit
detail_ulasan = pd.merge(data_ulasan,
                         rekap_rating_buku[['book_id', 'authors', 'title', 'original_publication_year']],
                         how='left', on='book_id')
detail_ulasan

"""# Content Based Filtering

## Data Preparation

**Mengatasi Missing Value**
"""

detail_ulasan.isnull().sum()

"""**Insight:** Terdapat missing value pada variabel authors sebanyak 88860317, title sebanyak 88860317, dan original_publication_year sebanyak 88870317.

"""

#menghapus data yang bernilai kosong
detail_ulasan = detail_ulasan.dropna()
detail_ulasan.isnull().sum()

"""**Insight:** Setelah dilakukan penghapusan pada missing value, kini sudah tidak terdapat missing value pada data."""

print('Jumlah judul buku: ', len(detail_ulasan.title.unique()))
print('Jumlah author: ', len(detail_ulasan.authors.unique()))
print('Jumlah tahun publikasi buku: ', len(detail_ulasan.original_publication_year.unique()))

"""**Menghapus data duplikat berdasarkan book_id**"""

# Hitung total duplikat berdasarkan kolom 'book_id'
jumlah_duplikat = detail_ulasan.duplicated(subset='book_id').sum()
jumlah_duplikat

"""**Insight:** Terdapat 7837104 duplikat berdasarkan variabel book_id"""

jumlah_duplikat = detail_ulasan[detail_ulasan.duplicated(subset='book_id', keep=False)]
print(jumlah_duplikat)

detail_ulasan = detail_ulasan.drop_duplicates('book_id')
detail_ulasan

# Mengubah kolom tertentu menjadi list untuk diproses lebih lanjut
daftar_id = detail_ulasan['book_id'].tolist()
daftar_judul = detail_ulasan['title'].tolist()
daftar_penulis = detail_ulasan['authors'].tolist()
daftar_tahun = detail_ulasan['original_publication_year'].tolist()

# Menyusun ulang data dalam bentuk DataFrame baru dengan nama kolom yang disesuaikan
rekap_buku = pd.DataFrame({
    'IDBuku': daftar_id,
    'JudulBuku': daftar_judul,
    'Penulis': daftar_penulis,
    'TahunTerbit': daftar_tahun
})
rekap_buku

"""## Model Development

**TF-IDF Vectorizer**
"""

# Inisialisasi dan pelatihan TF-IDF pada kolom penulis buku
vectorizer = TfidfVectorizer()
vectorizer.fit(rekap_buku['Penulis'])
vectorizer.get_feature_names_out()

# Transformasi data penulis menjadi representasi vektor TF-IDF
matrix_tfidf = vectorizer.fit_transform(rekap_buku['Penulis'])
matrix_tfidf.shape

"""**Insight:**  Matriks =berukuran (811, 1020). Nilai 811 merupakan ukuran data dan 1020 merupakan matrik kategori penulis."""

# Mengubah hasil vektor menjadi matriks densitas untuk keperluan visualisasi
dense_tfidf = matrix_tfidf.todense()

# Membuat DataFrame untuk melihat hasil TF-IDF, baris = judul buku, kolom = token penulis
df_tfidf = pd.DataFrame(
    dense_tfidf,
    index=rekap_buku['JudulBuku'],
    columns=vectorizer.get_feature_names_out()
)
df_tfidf.sample(22, axis=1).sample(10, axis=0)

"""**Cosine Similarity**"""

# Menghitung kesamaan antar buku berdasarkan penulis menggunakan cosine similarity
nilai_kemiripan = cosine_similarity(matrix_tfidf)
nilai_kemiripan

# Menyusun DataFrame untuk menampung hasil cosine similarity antar judul buku
df_kemiripan = pd.DataFrame(nilai_kemiripan,
                            index=rekap_buku['JudulBuku'],
                            columns=rekap_buku['JudulBuku'])
print('Dimensi Matriks:', df_kemiripan.shape)

# Menampilkan sampel kemiripan antar buku
df_kemiripan.sample(5, axis=1).sample(10, axis=0)

"""## Mendapatkan Rekomendasi
Membuat fungsi `book_reccomendations()`
"""

def rekomendasi_buku(nama_judul, data_kemiripan=df_kemiripan, info_buku=rekap_buku[['JudulBuku', 'Penulis']], jumlah=5):
    # Menemukan indeks dengan nilai kemiripan tertinggi terhadap buku yang dicari
    posisi_terdekat = data_kemiripan.loc[:, nama_judul].to_numpy().argpartition(
        range(-1, -(jumlah + 1), -1)
    )

    # Mengambil daftar judul buku dengan skor kemiripan tertinggi
    rekomendasi_teratas = data_kemiripan.columns[posisi_terdekat[-1:-(jumlah + 2):-1]]

    # Menghapus judul asli agar tidak muncul dalam hasil rekomendasi
    rekomendasi_teratas = rekomendasi_teratas.drop(nama_judul, errors='ignore')

    # Gabungkan dengan info buku untuk menampilkan hasil yang informatif
    return pd.DataFrame(rekomendasi_teratas).merge(info_buku).head(jumlah)

# Menampilkan detail buku yang dijadikan acuan pencarian
rekap_buku[rekap_buku['JudulBuku'] == 'East of Eden']

# Menampilkan daftar rekomendasi berdasarkan buku referensi
rekomendasi_buku('East of Eden')

"""**Insight:** Hasil rekomendasi menunjukkan buku dengan nama penulis sama atau hampir sama karena sistem rekomendasi dibuat berdasarkan penulis.

## Precision

Precision **mengukur seberapa banyak rekomendasi yang benar dari total yang diberikan**.

$$
\text{Precision@k} = \frac{\text{Jumlah item relevan dalam top-}k}{k}
$$


Fokus: Akurasi dari hasil yang ditampilkan ke user.

Hasil: Sistem merekomendasikan 5 penulis, dan 4 sesuai keinginan user berdasarkan input â†’

```
Input: 'East of Eden', penulis = John Steinbeck
Dataset:
0 - Of Mice and Men                             â†’ John Steinbeck âœ…  
1 - The Grapes of Wrath                         â†’ John Steinbeck âœ…  
2 - The Pearl	                               â†’ John Steinbeck âœ…  
3 - Travels with Charley: In Search of America  â†’ John Steinbeck âœ…
4 - The Summons                                 â†’ John Grisham âŒ
```


$$
Precision@5=\dfrac{4}{5}= 0.8
$$

80% rekomendasi yang diberikan sistem terbukti relevan.

# Collaborative Filtering

## Data Understanding
"""

# Menyalin data rating untuk digunakan dalam model rekomendasi
data_rating_baru = rating.copy()
data_rating_baru

"""## Data Preparation"""

# Menyiapkan encoding user_id menjadi indeks integer unik
daftar_user = data_rating_baru['user_id'].unique().tolist()
pemetaan_user = {user_id: idx for idx, user_id in enumerate(daftar_user)}
pemetaan_balikan = {idx: user_id for idx, user_id in enumerate(daftar_user)}

print(pemetaan_user)

# Mengubah book_id menjadi list unik dan melakukan proses encoding
daftar_buku = data_rating_baru['book_id'].unique().tolist()
kode_buku = {b: i for i, b in enumerate(daftar_buku)}
kode_balik_buku = {i: b for i, b in enumerate(daftar_buku)}

# Menentukan jumlah pengguna dalam data
jumlah_pengguna = len(pemetaan_user)
print(jumlah_pengguna)

# Menentukan jumlah buku yang tersedia
jumlah_buku = len(kode_balik_buku)
print(jumlah_buku)

# Mengonversi rating ke tipe float
data_rating_baru['rating'] = data_rating_baru['rating'].astype(np.float32)

# Menentukan nilai rating terendah dan tertinggi
rating_terkecil = data_rating_baru['rating'].min()
rating_terbesar = data_rating_baru['rating'].max()

print(f'Jumlah Pengguna: {jumlah_pengguna}, Jumlah Buku: {jumlah_buku}, Rating Minimum: {rating_terkecil}, Rating Maksimum: {rating_terbesar}')

"""**Membagi Data untuk Training dan Validasi**"""

# Mengacak urutan data untuk meminimalkan bias
data_rating_baru = data_rating_baru.sample(frac=1, random_state=42)
data_rating_baru

# Menyusun fitur input: kombinasi user_id dan book_id
fitur_input = data_rating_baru[['user_id', 'book_id']].values

# Menyusun target output: rating yang telah dinormalisasi
target_output = data_rating_baru['rating'].apply(lambda r: (r - rating_terkecil) / (rating_terbesar - rating_terkecil)).values

# Membagi data menjadi training set (80%) dan validation set (20%)
batas_latih = int(0.8 * data_rating_baru.shape[0])
x_train, x_val, y_train, y_val = (
    fitur_input[:batas_latih],
    fitur_input[batas_latih:],
    target_output[:batas_latih],
    target_output[batas_latih:]
)

print(fitur_input, target_output)

"""## Model Development"""

class JaringanRekomendasi(tf.keras.Model):

    def __init__(self, total_user, total_buku, dimensi_embedding, **kwargs):
        super(JaringanRekomendasi, self).__init__(**kwargs)
        self.total_user = total_user
        self.total_buku = total_buku
        self.dimensi_embedding = dimensi_embedding

        # Layer embedding untuk user
        self.embedding_user = layers.Embedding(
            total_user,
            dimensi_embedding,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.bias_user = layers.Embedding(total_user, 1)

        # Layer embedding untuk buku
        self.embedding_buku = layers.Embedding(
            total_buku,
            dimensi_embedding,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.bias_buku = layers.Embedding(total_buku, 1)

    def call(self, masukan):
        vektor_user = self.embedding_user(masukan[:, 0])
        bias_user = self.bias_user(masukan[:, 0])
        vektor_buku = self.embedding_buku(masukan[:, 1])
        bias_buku = self.bias_buku(masukan[:, 1])

        hasil_dot = tf.tensordot(vektor_user, vektor_buku, axes=2)
        prediksi = hasil_dot + bias_user + bias_buku

        return tf.nn.sigmoid(prediksi)

"""## Training Model"""

# Membuat instance dari model jaringan rekomendasi
model_rekomendasi = JaringanRekomendasi(jumlah_pengguna, jumlah_buku, 50)

# Menyusun konfigurasi model
model_rekomendasi.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Proses pelatihan model
riwayat_latih = model_rekomendasi.fit(
    x= x_train,
    y= y_train,
    batch_size=8,
    steps_per_epoch=10000,
    epochs=15,
    validation_data=(x_val, y_val)
)

"""# Visualisasi Metrik"""

# Plot history training model
plt.plot(riwayat_latih.history['root_mean_squared_error'])
plt.plot(riwayat_latih.history['val_root_mean_squared_error'])
plt.title('Metrik Model')
plt.ylabel('Root Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Latih', 'Validasi'], loc='upper left')
plt.show()

"""**Insight** Proses training model terlihat smooth dan model konvergen pada epochs sekitar 15. Dari proses ini, kita memperoleh nilai error akhir sebesar sekitar 0.2136 dan error pada data validasi sebesar 0.2163.

# Rekomendasi Menggunakan Collaborative Filtering
"""

# Asumsi: data buku ada di variabel 'data' (hasil transformasi sebelumnya)
book_df = rekap_buku.copy()

# Membaca data rating dari file CSV (pastikan path sudah benar)
rating = pd.read_csv(os.path.join(books_data_path, 'ratings.csv'))

# Sampling satu user secara acak
id_pembaca = rating['user_id'].sample(1).iloc[0]

# Buku yang sudah dibaca oleh user tersebut
book_read_by_user = rating[rating['user_id'] == id_pembaca]

# Buku yang belum dibaca user (filter buku yg ada di dataset dan sudah di-encode)
book_not_read = book_df[~book_df['IDBuku'].isin(book_read_by_user['book_id'].values)]['IDBuku']

# Ganti bagian ini:
book_not_read = list(set(book_not_read).intersection(set(kode_buku.keys())))

book_not_read_encoded = [[kode_buku[b]] for b in book_not_read]
user_encoded = pemetaan_user[id_pembaca]

# Array untuk input ke model
user_book_array = np.hstack((np.array([[user_encoded]] * len(book_not_read_encoded)), book_not_read_encoded))

# Prediksi rating buku yang belum dibaca oleh user
rate_buku = model_rekomendasi.predict(user_book_array).flatten()

# Ambil 10 buku dengan prediksi rating tertinggi
top_ratings_indices = rate_buku.argsort()[-10:][::-1]
recommended_book_ids = [kode_balik_buku[book_not_read_encoded[i][0]] for i in top_ratings_indices]

print(f'Menampilkan Rekomendasi Buku untuk User ID: {id_pembaca}')
print('===' * 12)
print('Buku yang paling tinggi ratingnya dari user tersebut:')
print('----' * 12)

# Buku yang paling disukai user berdasarkan rating asli
top_books_user = book_read_by_user.sort_values(by='rating', ascending=False).head(5)['book_id'].values
top_books_df = book_df[book_df['IDBuku'].isin(top_books_user)]

for row in top_books_df.itertuples():
    print(f"{row.penulis} : {row.judul_buku}")

print('----' * 12)
print('Top 10 Rekomendasi Buku:')
print('----' * 12)

# Tampilkan buku rekomendasi berdasarkan prediksi model
recommended_books_df = book_df[book_df['IDBuku'].isin(recommended_book_ids)]
for row in recommended_books_df.itertuples():
    print(f"{row.Penulis} : {row.JudulBuku}")

"""## Kesimpulan

Proyek ini berhasil membangun sistem rekomendasi buku yang efektif dengan menggabungkan pendekatan _Content Based Filtering_ dan  _Collaborative Filtering_. Pada model _Content Based Filtering_, sistem mampu memberikan rekomendasi yang relevan berdasarkan kemiripan penulis, dengan nilai precision mencapai 80%. Ini menunjukkan bahwa 4 dari 5 buku yang direkomendasikan benar-benar sesuai dengan preferensi pengguna.

Untuk model _Collaborative Filtering_, performa dievaluasi menggunakan _Root Mean Squared Error_ (RMSE) dimana nilai error akhir sebesar sekitar 0.2136 dan error pada data validasi sebesar 0.2163 . Hal ini menandakan bahwa model semakin akurat dalam memprediksi rating pengguna tanpa mengalami _overfitting_.


Secara keseluruhan, hasil evaluasi membuktikan bahwa sistem rekomendasi yang dikembangkan dalam proyek ini mampu bekerja secara optimal dan memberikan rekomendasi yang akurat serta relevan.

"""